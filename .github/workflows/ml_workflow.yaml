name: Get WandB Runs
on:
  issue_comment:
    types: [created] # Ensure this is the desired event type

jobs:
  get-runs:
    if: github.event.issue.pull_request != null && contains(github.event.comment.body, '/wandb_list_model')
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4 # Updated to v4
        
      - name: Get SHA from PR comment for context
        id: chatops
        uses: machine-learning-apps/actions-chatops@master # Consider checking for newer versions or alternatives if issues arise
        with:
          TRIGGER_PHRASE: "/wandb_list_model" # Command to trigger the action
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          
      - name: Setup Python
        uses: actions/setup-python@v5 # Updated to v5
        with:
          python-version: '3.9'
          
      - name: Install wandb
        run: pip install wandb
        
      - name: Create Python script for WandB reporting
        run: |
          cat > wandb_script.py << 'SCRIPT_EOF'
          import os
          import wandb
          from datetime import datetime, timedelta
          
          # Fetch environment variables
          entity = os.environ.get("WANDB_ENTITY")
          project = os.environ.get("WANDB_PROJECT")
          # TARGET_SHA is used for context in the report, not for filtering runs
          sha = os.environ.get("TARGET_SHA", "N/A") 
          
          if not entity or not project:
              print("Error: WANDB_ENTITY and WANDB_PROJECT environment variables must be set.")
              # Create a minimal markdown indicating the error
              with open("wandb_summary.md", "w") as f:
                  f.write(f"# ü§ñ ML Experiment Dashboard Error\n\n")
                  f.write("‚ùå **Configuration Error:** `WANDB_ENTITY` and/or `WANDB_PROJECT` are not set in GitHub secrets.\n")
              exit(0) # Exit gracefully so error report can be commented

          print(f"W&B Target: {entity}/{project}")
          print(f"Contextual Commit SHA: {sha}")
          
          api = wandb.Api(timeout=60) # Increased timeout for API calls
          
          # Fetch recent runs (e.g., last 7 days to manage scope)
          # W&B API filters by ISO format date string
          cutoff_date = datetime.utcnow() - timedelta(days=7) # Use UTCNow for consistency
          
          runs = []
          try:
              runs = api.runs(
                  path=f"{entity}/{project}", 
                  filters={"created_at": {"$gte": cutoff_date.isoformat()}},
                  order="-created_at" # Get newest runs first
              )
              print(f"Total runs fetched from the last 7 days: {len(runs)}")
          except Exception as e:
              print(f"Error fetching runs from W&B: {e}")
              with open("wandb_summary.md", "w") as f:
                  f.write(f"# ü§ñ ML Experiment Dashboard Error\n\n")
                  f.write(f"‚ùå **W&B API Error:** Failed to fetch runs for `{entity}/{project}`.\n")
                  f.write(f"   Error details: `{str(e)}`\n")
              exit(0) # Exit gracefully

          # Helper functions for formatting the report
          def get_performance_emoji(accuracy_value):
              if accuracy_value is None: return "‚ùì"
              if accuracy_value >= 0.95: return "üèÜ"
              if accuracy_value >= 0.90: return "ü•á"
              if accuracy_value >= 0.85: return "ü•à"
              if accuracy_value >= 0.80: return "ü•â"
              if accuracy_value >= 0.70: return "üëç"
              return "‚ö†Ô∏è"

          def format_runtime(seconds):
              if seconds is None: return "N/A"
              if not isinstance(seconds, (int, float)) or seconds < 0: return "Invalid"
              if seconds < 60: return f"{seconds:.1f}s"
              if seconds < 3600: return f"{seconds/60:.1f}m"
              return f"{seconds/3600:.1f}h"

          def get_experiment_insights(run_obj):
              # Safely get metrics, preferring validation metrics
              accuracy = run_obj.summary.get('best_val_acc') or run_obj.summary.get('val_accuracy') or run_obj.summary.get('accuracy')
              loss = run_obj.summary.get('best_val_loss') or run_obj.summary.get('val_loss') or run_obj.summary.get('loss')
              train_acc = run_obj.summary.get('train_accuracy')
              val_acc = run_obj.summary.get('val_accuracy') # Re-fetch for clarity if used directly
              runtime = run_obj.summary.get('_runtime')
              
              insights_list = []
              
              if accuracy is not None:
                  if accuracy >= 0.95: insights_list.append("üî• **Excellent performance** - Model is performing exceptionally well.")
                  elif accuracy >= 0.90: insights_list.append("‚ú® **Strong performance** - Model shows great results.")
                  elif accuracy >= 0.80: insights_list.append("üëç **Good performance** - Solid baseline results.")
                  elif accuracy >= 0.70: insights_list.append("‚ö° **Moderate performance** - Room for improvement.")
                  else: insights_list.append("‚ö†Ô∏è **Low performance** - Consider tuning or architecture changes.")
              
              if train_acc is not None and val_acc is not None:
                  gap = abs(train_acc - val_acc)
                  if gap > 0.15: insights_list.append(f"üö® **Potential overfitting** - Train/Val accuracy gap: {gap:.2f} ({train_acc:.3f} vs {val_acc:.3f}).")
                  elif gap > 0.05: insights_list.append(f"‚ö†Ô∏è **Slight overfitting** - Train/Val accuracy gap: {gap:.2f} ({train_acc:.3f} vs {val_acc:.3f}).")
                  else: insights_list.append(f"‚úÖ **Well-generalized** - Train/Val accuracy are close.")

              if runtime is not None:
                  if runtime < 300: insights_list.append(f"‚ö° **Fast training:** {format_runtime(runtime)}.")
                  elif runtime < 1800: insights_list.append(f"‚è±Ô∏è **Moderate training time:** {format_runtime(runtime)}.")
                  else: insights_list.append(f"üêå **Long training time:** {format_runtime(runtime)}.")
              
              # Model type hints based on name (case-insensitive)
              run_name_lower = run_obj.name.lower()
              if 'lstm' in run_name_lower or 'rnn' in run_name_lower: insights_list.append("üîÑ **Sequential model hint** (LSTM/RNN).")
              elif 'cnn' in run_name_lower or 'conv' in run_name_lower: insights_list.append("üñºÔ∏è **Convolutional model hint** (CNN).")
              elif 'transformer' in run_name_lower or 'bert' in run_name_lower: insights_list.append("ü§ñ **Transformer model hint**.")
              elif 'xgboost' in run_name_lower: insights_list.append("üå≥ **Gradient Boosting hint** (XGBoost).")
              elif 'random_forest' in run_name_lower or 'randomforest' in run_name_lower : insights_list.append("üå≤ **Random Forest hint**.")
              
              return insights_list if insights_list else ["No specific insights generated."]

          def generate_report():
              with open("wandb_summary.md", "w") as f:
                  f.write(f"# ü§ñ ML Experiment Dashboard for `{project}`\n")
                  context_sha_info = f"`{sha[:8]}`" if sha != "N/A" else "Not Available"
                  f.write(f"**Contextual Commit:** {context_sha_info} | **Fetched Runs (Last 7 Days):** {len(runs)}\n\n")
                  
                  if not runs:
                      f.write("‚ùå No recent runs found in the last 7 days for this project.\n")
                      f.write("\n---\n*Generated by WandB GitHub Action* ü§ñ")
                      return
                  
                  # Overall stats
                  finished_runs = [r for r in runs if r.state == "finished"]
                  best_run = max(finished_runs, key=lambda r: r.summary.get('best_val_acc') or r.summary.get('val_accuracy') or r.summary.get('accuracy') or -1, default=None)
                  
                  f.write("## üìä Quick Stats (Last 7 Days)\n")
                  if best_run and (best_run.summary.get('best_val_acc') or best_run.summary.get('val_accuracy') or best_run.summary.get('accuracy')) is not None:
                      best_acc_val = best_run.summary.get('best_val_acc') or best_run.summary.get('val_accuracy') or best_run.summary.get('accuracy')
                      f.write(f"üèÜ **Top Model (by Accuracy):** [{best_run.name}]({best_run.url}) ({best_acc_val:.3f})\n")
                  f.write(f"‚úÖ **Completed Runs:** {len(finished_runs)} / {len(runs)}\n")
                  f.write(f"üîÑ **Active Runs:** {len([r for r in runs if r.state == 'running'])}\n\n")
                  
                  # Leaderboard
                  f.write("## üèÅ Model Performance Leaderboard (Last 7 Days, Sorted by Accuracy)\n")
                  f.write("| Rank | Model | Run ID | Perf. | Accuracy | Loss | Runtime | Status |\n")
                  f.write("|------|-------|--------|-------|----------|------|---------|--------|\n")
                  
                  # Sort runs by a primary accuracy metric, then by creation date as a fallback.
                  # Handle cases where metrics might be missing.
                  def sort_key(run_obj):
                      acc = run_obj.summary.get('best_val_acc') or run_obj.summary.get('val_accuracy') or run_obj.summary.get('accuracy')
                      return (acc is not None, acc, run_obj.created_at)

                  sorted_for_leaderboard = sorted(runs, key=sort_key, reverse=True)

                  for idx, run_item in enumerate(sorted_for_leaderboard, 1):
                      acc = run_item.summary.get('best_val_acc') or run_item.summary.get('val_accuracy') or run_item.summary.get('accuracy')
                      loss = run_item.summary.get('best_val_loss') or run_item.summary.get('val_loss') or run_item.summary.get('loss')
                      rt = run_item.summary.get('_runtime')
                      
                      rank_str = f"{idx}"
                      if idx == 1: rank_str = "ü•á"
                      elif idx == 2: rank_str = "ü•à"
                      elif idx == 3: rank_str = "ü•â"

                      emoji = get_performance_emoji(acc)
                      acc_s = f"{acc:.3f}" if acc is not None else "N/A"
                      loss_s = f"{loss:.4f}" if loss is not None else "N/A"
                      rt_s = format_runtime(rt)
                      status_s = "‚úÖ Fin" if run_item.state == "finished" else ("üîÑ Run" if run_item.state == "running" else f"‚ö†Ô∏è {run_item.state.capitalize()}")
                      
                      f.write(f"| {rank_str} | **[{run_item.name}]({run_item.url})** | `{run_item.id[:8]}` | {emoji} | {acc_s} | {loss_s} | {rt_s} | {status_s} |\n")
                  f.write(f"\nPerformance Legend: üèÜ‚â•95% | ü•á‚â•90% | ÔøΩ‚â•85% | ü•â‚â•80% | üëç‚â•70% | ‚ö†Ô∏è<70% | ‚ùìUnknown\n\n")

                  # Detailed Analysis for each run
                  f.write(f"## üî¨ Detailed Experiment Analysis (Newest First)\n")
                  # Runs are already sorted newest first from the API query (order="-created_at")
                  for run_item in runs: # Iterate in fetched order (newest first)
                      acc = run_item.summary.get('best_val_acc') or run_item.summary.get('val_accuracy') or run_item.summary.get('accuracy')
                      
                      f.write(f"\n---\n### {get_performance_emoji(acc)} {run_item.name}\n")
                      f.write(f"**Run ID:** `{run_item.id}` | **Status:** {run_item.state} | **Created:** {run_item.created_at}\n")
                      
                      insights = get_experiment_insights(run_item)
                      f.write("**Insights:**\n")
                      for insight_text in insights: f.write(f"- {insight_text}\n")
                      
                      f.write("\n**Key Metrics:**\n")
                      core_metrics_display = {
                          "Accuracy (Val/Best)": acc,
                          "Loss (Val/Best)": run_item.summary.get('best_val_loss') or run_item.summary.get('val_loss') or run_item.summary.get('loss'),
                          "Train Accuracy": run_item.summary.get('train_accuracy'),
                          "Train Loss": run_item.summary.get('train_loss'),
                          "Runtime": run_item.summary.get('_runtime'),
                          "Epochs": run_item.summary.get('epoch') or run_item.summary.get('epochs'),
                      }
                      metrics_found_count = 0
                      for name, val in core_metrics_display.items():
                          if val is not None:
                              metrics_found_count += 1
                              if name == "Runtime": f.write(f"- **{name}:** {format_runtime(val)}\n")
                              elif isinstance(val, float): f.write(f"- **{name}:** {val:.4f}\n")
                              else: f.write(f"- **{name}:** {val}\n")
                      if not metrics_found_count: f.write("- *No standard metrics logged or available in summary.*\n")

                      config_data = run_item.config
                      if config_data:
                          f.write(f"\n**Configuration Highlights:**\n")
                          # Display a few important config keys if they exist
                          important_config_keys = ['model_type', 'optimizer', 'learning_rate', 'batch_size', 'epochs', 'dropout_rate']
                          config_found_count = 0
                          for key in important_config_keys:
                              if key in config_data:
                                  config_found_count +=1
                                  f.write(f"- **{key.replace('_', ' ').title()}:** `{config_data[key]}`\n")
                          if not config_found_count: f.write("- *No highlighted config parameters found.*\n")
                      
                      f.write(f"\n**Links:** [üìä Full Run Page]({run_item.url}) | [üìà Charts]({run_item.url}?workspace=user-{entity}) | [üìã Logs]({run_item.url}/logs)\n")

                  f.write("\n---\n")
                  f.write(f"*Report generated by WandB GitHub Action at {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S %Z')} UTC* ü§ñ\n")
                  f.write(f"*View all project runs: [W&B Dashboard](https://wandb.ai/{entity}/{project})*")

          generate_report()
          print("Enhanced WandB markdown report generated: wandb_summary.md")
          SCRIPT_EOF
          
      - name: Execute Python script for W&B report
        env:
          WANDB_API_KEY: ${{ secrets.WANDB_API_KEY }}
          WANDB_ENTITY: ${{ secrets.WANDB_ENTITY }} # Your W&B username or team name
          WANDB_PROJECT: ${{ secrets.WANDB_PROJECT }} # Your W&B project name
          TARGET_SHA: ${{ steps.chatops.outputs.SHA }} # SHA from PR comment for context
        run: python wandb_script.py
        
      - name: Debug - Display generated markdown content
        if: always() # Run this step even if previous steps fail, for debugging
        run: |
          echo "--- Generated Markdown Content (wandb_summary.md) ---"
          cat wandb_summary.md
          echo "--- End of Markdown Content ---"
          
      - name: Install GitHub CLI
        run: |
          sudo apt-get update -y && sudo apt-get install -y gh
          
      - name: Comment to PR with WandB Summary
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          PR_NUMBER: ${{ github.event.issue.number }}
        run: |
          echo "Attempting to comment on PR #$PR_NUMBER"
          gh pr comment $PR_NUMBER --body-file wandb_summary.md
          echo "Successfully commented on PR #$PR_NUMBER."
