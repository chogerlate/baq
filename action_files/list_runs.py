import os
import wandb
from datetime import datetime, timedelta

def get_performance_emoji(accuracy):
    """Get emoji based on accuracy performance"""
    if accuracy is None:
        return "â“"
    if accuracy >= 0.95:
        return "ğŸ†"
    elif accuracy >= 0.90:
        return "ğŸ¥‡"
    elif accuracy >= 0.85:
        return "ğŸ¥ˆ"
    elif accuracy >= 0.80:
        return "ğŸ¥‰"
    elif accuracy >= 0.70:
        return "ğŸ‘"
    else:
        return "âš ï¸"

def format_runtime(runtime_seconds):
    """Format runtime in a readable way"""
    if runtime_seconds is None:
        return "N/A"
    if runtime_seconds < 60:
        return f"{runtime_seconds:.1f}s"
    elif runtime_seconds < 3600:
        return f"{runtime_seconds/60:.1f}m"
    else:
        return f"{runtime_seconds/3600:.1f}h"

def get_metrics(run):
    """Extract single-step and multi-step metrics from run summary"""
    single_step = run.summary.get("single_step_metrics", {})
    multi_step = run.summary.get("multi_step_metrics", {})
    
    return {
        "single_step_accuracy": single_step.get("accuracy"),
        "single_step_mape": single_step.get("mape"),
        "single_step_mae": single_step.get("mae"),
        "multi_step_accuracy": multi_step.get("accuracy"),
        "multi_step_mape": multi_step.get("mape"),
        "multi_step_mae": multi_step.get("mae")
    }

def get_experiment_insights(run):
    """Generate experiment insights and comments for each model"""
    metrics = get_metrics(run)
    runtime = run.summary.get('_runtime')
    
    insights = []
    
    # Add insights based on metrics (condensed for brevity)
    single_acc = metrics["single_step_accuracy"]
    if single_acc is not None:
        if single_acc >= 0.95:
            insights.append("ğŸ”¥ **Excellent single-step performance** - Model predicts next step exceptionally well")
        elif single_acc >= 0.90:
            insights.append("âœ¨ **Strong single-step performance** - Great next-step prediction accuracy")
        elif single_acc >= 0.80:
            insights.append("ğŸ‘ **Good single-step performance** - Solid baseline for next-step prediction")
        elif single_acc >= 0.70:
            insights.append("âš¡ **Moderate single-step performance** - Room for improvement in next-step prediction")
        else:
            insights.append("âš ï¸ **Low single-step performance** - Consider model architecture changes for better next-step prediction")
    
    # Rest of the insights code (condensed for brevity)
    # ...
    
    return insights

def generate_report(runs, entity, project):
    """Generate the WandB runs report"""
    # Filter runs that have summary data
    filtered_runs = [run for run in runs if run.summary and 'single_step_metrics' in run.summary and 'multi_step_metrics' in run.summary]
    
    with open("wandb_summary.md", "w") as f:
        f.write(f"# ğŸ”® Time Series Prediction Dashboard\n")
        f.write(f"**Total Recent Runs:** {len(filtered_runs)} | **Time Range:** Last 7 days\n\n")
        
        if not filtered_runs:
            f.write("âŒ No recent runs found with complete metrics in the last 7 days.\n\n")
            f.write("---\n*Generated by WandB GitHub Action* ğŸ¤–")
            return
        
        best_single_accuracy = 0
        best_multi_accuracy = 0
        best_single_model = None
        best_multi_model = None
        best_single_run_id = None
        best_multi_run_id = None
        total_runtime = 0
        finished_runs = [r for r in filtered_runs if r.state == "finished"]
        
        for run in finished_runs:
            metrics = get_metrics(run)
            runtime = run.summary.get('_runtime')
            
            if metrics["single_step_accuracy"] and metrics["single_step_accuracy"] > best_single_accuracy:
                best_single_accuracy = metrics["single_step_accuracy"]
                best_single_model = run.name
                best_single_run_id = run.id
            
            if metrics["multi_step_accuracy"] and metrics["multi_step_accuracy"] > best_multi_accuracy:
                best_multi_accuracy = metrics["multi_step_accuracy"]
                best_multi_model = run.name
                best_multi_run_id = run.id
            
            if runtime:
                total_runtime += runtime
        
        f.write("## ğŸ“Š Quick Stats\n\n")
        if best_single_model and best_single_accuracy > 0:
            f.write(f"ğŸ¥‡ **Best Single-Step Model:** [{best_single_model}](https://wandb.ai/{entity}/{project}/runs/{best_single_run_id}) ({best_single_accuracy:.3f} accuracy)\n")
        if best_multi_model and best_multi_accuracy > 0:
            f.write(f"ğŸ¯ **Best Multi-Step Model:** [{best_multi_model}](https://wandb.ai/{entity}/{project}/runs/{best_multi_run_id}) ({best_multi_accuracy:.3f} accuracy)\n")
        f.write(f"â±ï¸ **Total Training Time:** {format_runtime(total_runtime)}\n")
        f.write(f"âœ… **Completed Runs:** {len(finished_runs)}/{len(filtered_runs)}\n")
        f.write(f"ğŸ”„ **Active Runs:** {len([r for r in filtered_runs if r.state == 'running'])}\n\n")
        
        f.write("## ğŸ Model Performance Leaderboard\n\n")
        f.write("| Rank | Model | Run ID | Single-Step Acc | Multi-Step Acc | Single MAPE | Multi MAPE | Runtime | Status |\n")
        f.write("|------|-------|--------|-----------------|----------------|-------------|------------|---------|--------|\n")
        
        sorted_runs = sorted(filtered_runs, key=lambda x: get_metrics(x)["single_step_accuracy"] or 0, reverse=True)
        
        for idx, run in enumerate(sorted_runs, 1):
            metrics = get_metrics(run)
            runtime = run.summary.get('_runtime')
            
            perf_emoji = get_performance_emoji(metrics["single_step_accuracy"])
            single_acc_str = f"{metrics['single_step_accuracy']:.3f}" if metrics["single_step_accuracy"] is not None else "N/A"
            multi_acc_str = f"{metrics['multi_step_accuracy']:.3f}" if metrics["multi_step_accuracy"] is not None else "N/A"
            single_mape_str = f"{metrics['single_step_mape']:.2f}%" if metrics["single_step_mape"] is not None else "N/A"
            multi_mape_str = f"{metrics['multi_step_mape']:.2f}%" if metrics["multi_step_mape"] is not None else "N/A"
            runtime_str = format_runtime(runtime)
            
            status_emoji = "âœ…" if run.state == "finished" else "ğŸ”„" if run.state == "running" else "âŒ"
            
            rank_emoji = "ğŸ¥‡" if idx == 1 else "ğŸ¥ˆ" if idx == 2 else "ğŸ¥‰" if idx == 3 else f"{idx}"
            
            f.write(f"| {rank_emoji} | **{run.name}** | `{run.id[:8]}` | {single_acc_str} | {multi_acc_str} | {single_mape_str} | {multi_mape_str} | {runtime_str} | {status_emoji} |\n")
        
        f.write(f"\nğŸ“ˆ **Performance Legend:** ğŸ† 95%+ | ğŸ¥‡ 90%+ | ğŸ¥ˆ 85%+ | ğŸ¥‰ 80%+ | ğŸ‘ 70%+ | âš ï¸ <70%\n\n")
        
        f.write(f"## ğŸ”¬ Detailed Experiment Analysis\n\n")
        
        for idx, run in enumerate(sorted_runs, 1):
            metrics = get_metrics(run)
            
            f.write(f"### {get_performance_emoji(metrics['single_step_accuracy'])} {run.name}\n")
            f.write(f"**Run ID:** `{run.id}` | **Status:** {run.state} | **Created:** {run.created_at}\n\n")
            
            insights = get_experiment_insights(run)
            if insights:
                f.write("**ğŸ” Experiment Insights:**\n")
                for insight in insights:
                    f.write(f"- {insight}\n")
                f.write("\n")
            
            f.write("**ğŸ“Š Key Metrics:**\n")
            
            f.write("*Single-Step Prediction:*\n")
            if metrics["single_step_accuracy"] is not None:
                f.write(f"- **Accuracy:** {metrics['single_step_accuracy']:.4f}\n")
            if metrics["single_step_mape"] is not None:
                f.write(f"- **MAPE:** {metrics['single_step_mape']:.4f}%\n")
            if metrics["single_step_mae"] is not None:
                f.write(f"- **MAE:** {metrics['single_step_mae']:.4f}\n")
            
            f.write("\n*Multi-Step Prediction:*\n")
            if metrics["multi_step_accuracy"] is not None:
                f.write(f"- **Accuracy:** {metrics['multi_step_accuracy']:.4f}\n")
            if metrics["multi_step_mape"] is not None:
                f.write(f"- **MAPE:** {metrics['multi_step_mape']:.4f}%\n")
            if metrics["multi_step_mae"] is not None:
                f.write(f"- **MAE:** {metrics['multi_step_mae']:.4f}\n")
            
            runtime = run.summary.get('_runtime')
            if runtime:
                f.write(f"\n*Training Information:*\n")
                f.write(f"- **Training Time:** {format_runtime(runtime)}\n")
            
            if all(v is None for v in metrics.values()) and runtime is None:
                f.write("- *No detailed metrics available*\n")
            
            config = run.config
            if config:
                f.write(f"\n**âš™ï¸ Configuration:**\n")
                important_configs = ['model_type', 'architecture', 'optimizer', 'learning_rate', 'batch_size', 'epochs', 'sequence_length', 'prediction_horizon', 'hidden_size']
                config_found = False
                for key in important_configs:
                    if key in config:
                        config_found = True
                        f.write(f"- **{key.replace('_', ' ').title()}:** {config[key]}\n")
                if not config_found:
                    f.write("- *No configuration details available*\n")
            
            # Quick actions
            f.write(f"\n**ğŸ”— Quick Actions:**\n")
            f.write(f"- [ğŸ“Š View Full Run](https://wandb.ai/{entity}/{project}/runs/{run.id})\n")
            f.write(f"- [ğŸ“ˆ View Charts](https://wandb.ai/{entity}/{project}/runs/{run.id}?workspace=user-{entity})\n")
            f.write(f"- [ğŸ“‹ View Logs](https://wandb.ai/{entity}/{project}/runs/{run.id}/logs)\n")
            
            # Recommendations
            single_acc = metrics["single_step_accuracy"]
            multi_acc = metrics["multi_step_accuracy"]
            if single_acc is not None or multi_acc is not None:
                f.write(f"\n**ğŸ’¡ Recommendations:**\n")
                if (single_acc and single_acc < 0.7) or (multi_acc and multi_acc < 0.7):
                    f.write("- Consider increasing model complexity or sequence length\n")
                    f.write("- Try different hyperparameters (learning rate, batch size)\n")
                    f.write("- Review data preprocessing and feature engineering\n")
                elif (single_acc and single_acc < 0.85) or (multi_acc and multi_acc < 0.8):
                    f.write("- Fine-tune hyperparameters for better performance\n")
                    f.write("- Consider attention mechanisms or more advanced architectures\n")
                    f.write("- Experiment with different prediction horizons\n")
                else:
                    f.write("- Excellent performance! Consider model deployment\n")
                    f.write("- Document successful configuration for future experiments\n")
                    f.write("- Test on different datasets to validate generalization\n")
            
            f.write("\n---\n\n")
        
        f.write(f"## ğŸ¯ Summary & Next Steps\n\n")
        if best_single_model:
            f.write(f"ğŸ¥‡ **Best single-step model:** {best_single_model} with {best_single_accuracy:.3f} accuracy\n")
        if best_multi_model:
            f.write(f"ğŸ¯ **Best multi-step model:** {best_multi_model} with {best_multi_accuracy:.3f} accuracy\n\n")
        
        running_count = len([r for r in filtered_runs if r.state == 'running'])
        if running_count > 0:
            f.write(f"â³ **{running_count} experiments still running** - Check back later for complete results\n\n")
        
        failed_count = len([r for r in filtered_runs if r.state == 'failed'])
        if failed_count > 0:
            f.write(f"âŒ **{failed_count} experiments failed** - Review logs for debugging\n\n")
        
        f.write("**Suggested Next Steps:**\n")
        f.write("- Compare single-step vs multi-step performance patterns\n")
        f.write("- Analyze error metrics (MAPE, MAE) for prediction quality insights\n")
        f.write("- Consider ensemble methods combining best single and multi-step models\n")
        f.write("- Plan production deployment for best performing models\n")
        f.write("- Document successful architectures and hyperparameters\n\n")
        
        f.write("---\n")
        f.write(f"*Generated by WandB GitHub Action at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} UTC* ğŸ¤–\n")
        f.write(f"*View all runs: [WandB Dashboard](https://wandb.ai/{entity}/{project})*")


def main():
    entity = os.environ["WANDB_ENTITY"]
    project = os.environ["WANDB_PROJECT"]
    
    api = wandb.Api()
    
    # Fetch recent runs (last 7 days to avoid too many API calls)
    cutoff_date = datetime.now() - timedelta(days=7)
    runs = api.runs(f"{entity}/{project}", 
                  filters={"created_at": {"$gte": cutoff_date.isoformat()}})
    
    print(f"Total runs fetched: {len(runs)}")
    
    # Display all runs with their run IDs and basic info
    for r in runs:
        print(f"Run ID: {r.id}, Name: {r.name}, State: {r.state}, Created: {r.created_at}")
    
    generate_report(runs, entity, project)
    print("Enhanced markdown file generated successfully")

if __name__ == "__main__":
    main()
